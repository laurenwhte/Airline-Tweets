{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Written by Lauren White\n",
    "# CS256T_2004d Foundations of Big Data Analytics\n",
    "# December 12, 2020\n",
    "\n",
    "# This program reads the content of the 'text' column from\n",
    "# the provided dataset, then cleans and tokenizes that data.\n",
    "\n",
    "# Import and indicate target data - only interested in 'text' column.\n",
    "data = pd.read_csv(r'Tweets.csv')\n",
    "\n",
    "# Getting info on params for CSV - NOTE: text_clean column was not originally\n",
    "# present, but has been added through testing of process_tweets function.\n",
    "print(\"Size of dataset: \", data.shape)\n",
    "print(\"Columns are: \", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(input_text):\n",
    "    \n",
    "    # Remove @s, #s, and URLs.\n",
    "    remtags = re.sub(r'@\\w+','',input_text)\n",
    "    remhash = re.sub(r'#','', remtags)\n",
    "    remurls = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', remhash)\n",
    "    \n",
    "    # Remove any remaining unrelated characters.\n",
    "    whitelist = string.ascii_letters + string.digits + ' '\n",
    "    remsymbols = ''.join(char for char in remurls if char in whitelist)\n",
    "  \n",
    "    # Reduce words to their base form (lemmatize) & remove punctuation\n",
    "    punctuations = string.punctuation + '...' # Define punctuation marks\n",
    "    nlp = spacy.load('en_core_web_sm') # Load spacy\n",
    "    lem = nlp(remhash) # Apply spacy\n",
    "    \n",
    "    lemmed=[]\n",
    "    for word in lem:\n",
    "        word = word.lemma_.lower().strip()\n",
    "        if ((word not in punctuations) & (word != '-pron-')):\n",
    "            lemmed.append(word)\n",
    "   \n",
    "    # Remove stopwords & return clean, tokenized tweet.\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    cleaned_tweet=[word for word in lemmed if word not in stop_words]\n",
    "    return cleaned_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the process_tweets function to the data and save to a new column in the CSV.\n",
    "data['text_clean'] = (data['text'].apply(process_tweets))\n",
    "data.to_csv(r'Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to pivot the data and create new variables to group all \n",
    "# tokenized Tweets by airline.\n",
    "airlines = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Generate wordclouds by airline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
