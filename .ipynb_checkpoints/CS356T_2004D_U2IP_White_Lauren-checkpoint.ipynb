{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 18 columns):\n",
      "Unnamed: 0                      14640 non-null int64\n",
      "Unnamed: 0.1                    14640 non-null int64\n",
      "tweet_id                        14640 non-null float64\n",
      "airline_sentiment               14640 non-null object\n",
      "airline_sentiment_confidence    14640 non-null float64\n",
      "negativereason                  9178 non-null object\n",
      "negativereason_confidence       10522 non-null float64\n",
      "airline                         14640 non-null object\n",
      "airline_sentiment_gold          40 non-null object\n",
      "name                            14640 non-null object\n",
      "negativereason_gold             32 non-null object\n",
      "retweet_count                   14640 non-null int64\n",
      "text                            14640 non-null object\n",
      "tweet_coord                     1019 non-null object\n",
      "tweet_created                   14640 non-null object\n",
      "tweet_location                  9907 non-null object\n",
      "user_timezone                   9820 non-null object\n",
      "text_clean                      20 non-null object\n",
      "dtypes: float64(3), int64(3), object(12)\n",
      "memory usage: 2.0+ MB\n",
      "Size of dataset:  (14640, 18)\n",
      "Columns are:  Index(['Unnamed: 0', 'Unnamed: 0.1', 'tweet_id', 'airline_sentiment',\n",
      "       'airline_sentiment_confidence', 'negativereason',\n",
      "       'negativereason_confidence', 'airline', 'airline_sentiment_gold',\n",
      "       'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord',\n",
      "       'tweet_created', 'tweet_location', 'user_timezone', 'text_clean'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Written by Lauren White\n",
    "# CS256T_2004d Foundations of Big Data Analytics\n",
    "# December 12, 2020\n",
    "\n",
    "# This program reads the content of the 'text' column from\n",
    "# the provided dataset, then cleans and tokenizes that data.\n",
    "\n",
    "# Import and indicate target data - only interested in 'text' column.\n",
    "data = pd.read_csv(r'Tweets.csv')\n",
    "\n",
    "# Getting info on params for CSV - NOTE: text_clean column was not originally\n",
    "# present, but has been added through testing of process_tweets function.\n",
    "print(\"Size of dataset: \", data.shape)\n",
    "print(\"Columns are: \", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(input_text):\n",
    "    \n",
    "    # Remove @s, #s, and URLs.\n",
    "    remtags = re.sub(r'@\\w+','',input_text)\n",
    "    remhash = re.sub(r'#','', remtags)\n",
    "    remurls = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', remhash)\n",
    "    \n",
    "    # Remove any remaining unrelated characters.\n",
    "    whitelist = string.ascii_letters + string.digits + ' '\n",
    "    remsymbols = ''.join(char for char in remurls if char in whitelist)\n",
    "  \n",
    "    # Reduce words to their base form (lemmatize) & remove punctuation\n",
    "    punctuations = string.punctuation + '...' # Define punctuation marks\n",
    "    nlp = spacy.load('en_core_web_sm') # Load spacy\n",
    "    lem = nlp(remhash) # Apply spacy\n",
    "    \n",
    "    lemmed=[]\n",
    "    for word in lem:\n",
    "        word = word.lemma_.lower().strip()\n",
    "        if ((word not in punctuations) & (word != '-pron-')):\n",
    "            lemmed.append(word)\n",
    "   \n",
    "    # Remove stopwords & return clean, tokenized tweet.\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    cleaned_tweet=[word for word in lemmed if word not in stop_words]\n",
    "    return cleaned_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the process_tweets function to the data and save to a new column in the CSV.\n",
    "data['text_clean'] = (data['text'].apply(process_tweets))\n",
    "data.to_csv(r'Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Group the data by airlines and append those token lists together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Generate wordclouds by airline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
